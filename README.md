# @aid-on/unillm

**çœŸã®ã‚¨ãƒƒã‚¸ãƒã‚¤ãƒ†ã‚£ãƒ–çµ±ä¸€LLMãƒ©ã‚¤ãƒ–ãƒ©ãƒª** - è»½é‡ä¾å­˜ï¼ˆZod ã®ã¿ï¼‰ã€WebStreamsã€ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã§ã‚¨ãƒƒã‚¸ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ç’°å¢ƒã«ç‰¹åŒ–ã€‚

## ğŸš€ ãªãœ unillmï¼Ÿ

### âš¡ ã‚¨ãƒƒã‚¸æœ€é©åŒ–è¨­è¨ˆ
- **è»½é‡ãƒãƒ³ãƒ‰ãƒ«**: ~50KB (vs AI SDK ~200KB+)
- **ç¬æ™‚èµ·å‹•**: Cold start ~10ms (vs ~50ms+)
- **æœ€å°ä¾å­˜**: Zod ã®ã¿ (~11KB)
- **WebStreams**: ReadableStream ãƒã‚¤ãƒ†ã‚£ãƒ–å¯¾å¿œ

### ğŸ”„ çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
```typescript
// ã™ã¹ã¦ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§åŒã˜API
await generate("groq:llama-3.1-8b-instant", messages, credentials);
await generate("gemini:gemini-2.0-flash", messages, credentials);
await generate("cloudflare:@cf/openai/gpt-oss-120b", messages, credentials);
```

### ğŸ¯ æ§‹é€ åŒ–å‡ºåŠ›
```typescript
// Fluent API
const person = await unilmp()
  .model("groq:llama-3.1-8b-instant")
  .credentials({ groqApiKey: "..." })
  .schema(z.object({ name: z.string(), age: z.number() }))
  .generate("Generate a person");

console.log(person.object.name); // å‹å®‰å…¨
```

## ğŸ“¦ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
npm install @aid-on/unilmp
```

## ğŸŒŠ nagare Stream<T> çµ±åˆ

unilmp ã¯ **@aid-on/nagare** ã® `Stream<T>` ã‚’è¿”ã™ãŸã‚ã€ä»–ã® Aid-On ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨å®Œå…¨ãªäº’æ›æ€§ãŒã‚ã‚Šã¾ã™ã€‚

```typescript
import { unilmp } from "@aid-on/unilmp";
import type { Stream } from "@aid-on/nagare";

// stream() ãƒ¡ã‚½ãƒƒãƒ‰ã¯ nagare Stream<string> ã‚’è¿”ã™
const stream: Stream<string> = await unilmp()
  .model("groq:llama-3.3-70b-versatile")
  .credentials({ groqApiKey: "..." })
  .stream("Tell me a story");

// nagare ã® Fluent API ãŒä½¿ãˆã‚‹
const enhanced = stream
  .map(chunk => chunk.toUpperCase())
  .filter(chunk => chunk.length > 0)
  .tap(chunk => console.log(`Streaming: ${chunk}`))
  .throttle(16)  // ~60fps
  .toSSE();      // Server-Sent Events ã«å¤‰æ›

// Qwik components ã¨ã®çµ±åˆ (@aid-on/qwiks)
import { useStreamText } from "@aid-on/qwiks";

const aiResponse = useStreamText(async () => {
  return await unilmp()
    .model("groq:llama-3.3-70b-versatile")
    .credentials({ groqApiKey })
    .stream("Hello");
});
```

## ğŸƒâ€â™‚ï¸ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### ğŸš€ Fluent Builder API (æ¨å¥¨)

```typescript
import { unilmp } from "@aid-on/unilmp";

// ãƒ¢ãƒ€ãƒ³ãªãƒã‚§ã‚¤ãƒ³å¼API
const result = await unilmp()
  .model("groq:llama-3.1-8b-instant")
  .credentials({ groqApiKey: process.env.GROQ_API_KEY })
  .temperature(0.7)
  .generate("Hello in Japanese");

console.log(result.text); // "ã“ã‚“ã«ã¡ã¯"
```

### âš¡ ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆ

```typescript
import { groq, gemini, cloudflare } from "@aid-on/unilmp";

// è¶…ç°¡æ½”ãªè¨˜æ³•
const result1 = await groq.instant(apiKey).generate("Hello");
const result2 = await gemini.flash(apiKey).generate("Hello");
const result3 = await cloudflare.gpt120b(creds).generate("Hello");
```

### ğŸ”§ å¾“æ¥API (ä¸‹ä½äº’æ›)

```typescript
import { generate } from "@aid-on/unilmp";

// å¾“æ¥ã®é–¢æ•°å‹API
const result = await generate(
  "groq:llama-3.1-8b-instant",
  [{ role: "user", content: "Hello in Japanese" }],
  { groqApiKey: process.env.GROQ_API_KEY }
);
```

## ğŸ“‹ å¯¾å¿œãƒ¢ãƒ‡ãƒ« (38ãƒ¢ãƒ‡ãƒ«ç¶²ç¾…)

### ğŸš€ Groq (7ãƒ¢ãƒ‡ãƒ«) - é«˜é€Ÿãƒ»ã‚³ã‚¹ãƒ‘æœ€å¼·
```typescript
// ãƒ¡ã‚¤ãƒ³LLM
"groq:llama-3.1-8b-instant"      // âš¡560 tokens/sec æœ€é«˜é€Ÿ
"groq:llama-3.3-70b-versatile"   // ğŸ§ 280 tokens/sec ãƒãƒ©ãƒ³ã‚¹
"groq:openai/gpt-oss-120b"       // ğŸ†æœ€é«˜æ€§èƒ½ 120B
"groq:openai/gpt-oss-20b"        // ğŸš€è»½é‡é«˜æ€§èƒ½ 20B

// ç‰¹æ®Šç”¨é€”
"groq:meta-llama/llama-guard-4-12b"  // ğŸ›¡ï¸ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒã‚§ãƒƒã‚¯
"groq:groq/compound"             // ğŸŒWebæ¤œç´¢+ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œ
"groq:groq/compound-mini"        // ğŸŒè»½é‡ç‰ˆWebæ¤œç´¢+ã‚³ãƒ¼ãƒ‰

// Fluent API
await groq.instant(key).generate("è¶…é«˜é€Ÿå¿œç­”");
await groq.compound(key).generate("Webæ¤œç´¢ã—ã¦èª¿ã¹ã¦");
```

### ğŸ§  Google Gemini (9ãƒ¢ãƒ‡ãƒ«) - é«˜å“è³ªãƒ»ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«
```typescript
// Gemini 3 (æœ€æ–°)
"gemini:gemini-3-pro-preview"    // ğŸ¥‡æœ€é«˜å“è³ª (Preview)
"gemini:gemini-3-flash-preview"  // âš¡é«˜é€Ÿç‰ˆ (Preview)

// Gemini 2.5 (é«˜æ€§èƒ½)
"gemini:gemini-2.5-pro"          // ğŸ†ãƒ—ãƒ­ç‰ˆ
"gemini:gemini-2.5-flash"        // âš¡é«˜é€Ÿç‰ˆ
"gemini:gemini-2.5-flash-lite"   // ğŸª¶è»½é‡ç‰ˆ

// Gemini 2.0 (æ¨å¥¨)
"gemini:gemini-2.0-flash"        // âš¡æ¨å¥¨ãƒ»å®‰å®šç‰ˆ
"gemini:gemini-2.0-flash-lite"   // ğŸª¶è¶…è»½é‡ç‰ˆ

// Gemini 1.5 (å®‰å®šç‰ˆ)
"gemini:gemini-1.5-pro-002"      // ğŸ›ï¸å®‰å®šãƒ—ãƒ­ç‰ˆ
"gemini:gemini-1.5-flash-002"    // âš¡å®‰å®šé«˜é€Ÿç‰ˆ

// Fluent API
await gemini.flash3(key).generate("æœ€æ–°Gemini 3");
await gemini.pro25(key).generate("æœ€é«˜å“è³ª");
await gemini.flash(key).generate("æ¨å¥¨å®‰å®šç‰ˆ");
```

### â˜ï¸ Cloudflare Workers AI (22ãƒ¢ãƒ‡ãƒ«) - ç„¡æ–™ãƒ»ã‚¨ãƒƒã‚¸æœ€é©åŒ–
```typescript
// OpenAI Models
"cloudflare:@cf/openai/gpt-oss-120b"  // ğŸ†æœ€é«˜æ€§èƒ½
"cloudflare:@cf/openai/gpt-oss-20b"   // ğŸš€é«˜é€Ÿç‰ˆ

// Meta Llama Models  
"cloudflare:@cf/meta/llama-4-scout-17b-16e-instruct"     // ğŸ¨ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«
"cloudflare:@cf/meta/llama-3.3-70b-instruct-fp8-fast"   // âš¡é‡å­åŒ–é«˜é€Ÿ
"cloudflare:@cf/meta/llama-3.1-70b-instruct"            // ğŸ§ å¤§å‹ç‰ˆ
"cloudflare:@cf/meta/llama-3.1-8b-instruct-fast"        // ğŸš€æœ€é©åŒ–ç‰ˆ
"cloudflare:@cf/meta/llama-3.1-8b-instruct"             // ğŸ“±è»½é‡ç‰ˆ

// ä¼æ¥­ãƒ»ç‰¹æ®Šç”¨é€”
"cloudflare:@cf/ibm/granite-4.0-h-micro"                // ğŸ¢ä¼æ¥­å‘ã‘
"cloudflare:@cf/mistralai/mistral-small-3.1-24b-instruct"  // ğŸ‡«ğŸ‡·Mistral 24B
"cloudflare:@cf/mistralai/mistral-7b-instruct-v0.2"     // ğŸ‡«ğŸ‡·Mistral 7B
"cloudflare:@cf/google/gemma-3-12b-it"                  // ğŸŒå¤šè¨€èªå¯¾å¿œ
"cloudflare:@cf/qwen/qwq-32b"                           // ğŸ¤”æ¨è«–ç‰¹åŒ–
"cloudflare:@cf/qwen/qwen2.5-coder-32b-instruct"       // ğŸ’»ã‚³ãƒ¼ãƒ‰ç‰¹åŒ–
"cloudflare:@cf/qwen/qwen3-30b-a3b-fp8"                 // ğŸ‡¨ğŸ‡³æœ€æ–°Qwen

// Fluent API
await cloudflare.gpt120b(creds).generate("æœ€é«˜æ€§èƒ½");
await cloudflare.llama4(creds).generate("ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«");  
await cloudflare.reasoning(creds).generate("æ¨è«–å•é¡Œ");
await cloudflare.coder(creds).generate("ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ");
```

## ğŸ”§ èªè¨¼è¨­å®š

```typescript
import { getCredentialsFromEnv } from "@aid-on/unilmp";

// ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è‡ªå‹•å–å¾—
const credentials = getCredentialsFromEnv();
// èª­ã¿è¾¼ã¿: GROQ_API_KEY, GEMINI_API_KEY,
//          CLOUDFLARE_API_KEY, CLOUDFLARE_EMAIL, CLOUDFLARE_ACCOUNT_ID

// ã¾ãŸã¯ç›´æ¥æŒ‡å®š
const credentials = {
  groqApiKey: "gsk_...",
  geminiApiKey: "AIza...",
  cloudflareApiKey: "...",
  cloudflareEmail: "you@example.com",
  cloudflareAccountId: "...",
};
```

## ğŸŒŠ ã‚¨ãƒƒã‚¸ç’°å¢ƒã§ã®ä½¿ç”¨

### Cloudflare Workers

```typescript
import { cloudflare } from "@aid-on/unilmp";

export default {
  async fetch(request: Request, env: Env): Promise<Response> {
    const { prompt } = await request.json();
    
    // Fluent API ã§ã‚·ãƒ³ãƒ—ãƒ«ã«
    const result = await cloudflare
      .gpt120b({
        apiKey: env.CLOUDFLARE_API_KEY,
        email: env.CLOUDFLARE_EMAIL,
        accountId: env.CLOUDFLARE_ACCOUNT_ID,
      })
      .temperature(0.7)
      .system("You are a helpful assistant")
      .generate(prompt);
    
    return Response.json({ 
      response: result.text,
      usage: result.usage 
    });
  }
};
```

### Vercel Edge Functions

```typescript
import { groq } from "@aid-on/unilmp";

export const config = { runtime: 'edge' };

export default async function handler(req: Request) {
  const { message } = await req.json();
  
  // è¶…ç°¡æ½”ãªAPI
  const result = await groq
    .versatile(process.env.GROQ_API_KEY!)
    .temp(0.5)
    .generate(message);
  
  return new Response(result.text);
}
```

### Next.js App Router (Edge)

```typescript
import { unilmp } from "@aid-on/unilmp";
import { z } from "zod";

export const runtime = 'edge';

export async function POST(request: Request) {
  const { prompt } = await request.json();
  
  // æ§‹é€ åŒ–å¿œç­” + Fluent API
  const analysis = await unilmp()
    .model("groq:llama-3.3-70b-versatile")
    .credentials({ groqApiKey: process.env.GROQ_API_KEY! })
    .system("You are a sentiment analyzer")
    .temp(0.3)
    .schema(z.object({
      sentiment: z.enum(["positive", "negative", "neutral"]),
      confidence: z.number().min(0).max(1),
      summary: z.string(),
    }))
    .generate(prompt);
  
  return Response.json(analysis.object);
}
```

## ğŸ” ãƒªãƒˆãƒ©ã‚¤ãƒ»ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

```typescript
import { unilmp, groq, gemini } from "@aid-on/unilmp";

// Fluent API ã§ãƒªãƒˆãƒ©ã‚¤
const result = await unilmp()
  .model("groq:llama-3.1-8b-instant")
  .credentials(creds)
  .retries(3, 1000) // 3å›ã€1ç§’é–“éš”
  .generate("Hello");

// ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
async function generateWithFallback(prompt: string, creds: any) {
  try {
    return await groq.instant(creds.groqApiKey).generate(prompt);
  } catch (error) {
    console.warn("Groq failed, trying Gemini...");
    return await gemini.flash(creds.geminiApiKey).generate(prompt);
  }
}

// ä¼šè©±å‹ãƒã‚§ã‚¤ãƒ³
const conversation = await unilmp()
  .model("groq:llama-3.3-70b-versatile")
  .credentials(creds)
  .system("You are a helpful assistant")
  .user("Hello")
  .assistant("Hi! How can I help?")
  .user("What's 2+2?")
  .temp(0.3)
  .retries(2)
  .generate();
```

## ğŸ’¾ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–

```typescript
import { unilmp, EdgeCache, StreamingBuffer } from "@aid-on/unilmp";

// Fluent API ã§ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
const result = await unilmp()
  .model("groq:llama-3.3-70b-versatile")
  .credentials(creds)
  .messages(longConversationHistory) // 100å€‹ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
  .optimize(2000) // 2000ãƒˆãƒ¼ã‚¯ãƒ³ä»¥å†…ã«è‡ªå‹•å‰Šæ¸›
  .compress() // ç©ºç™½æ–‡å­—ã‚’è‡ªå‹•åœ§ç¸®
  .system("You are helpful")
  .generate("Continue the conversation");

// æ‰‹å‹•æœ€é©åŒ–ã‚‚å¯èƒ½
import { truncateMessages, compressMessage } from "@aid-on/unilmp";

const optimized = truncateMessages(longHistory, 2000);
const compressed = compressMessage("Hello    world\n\n\nHow are you?");

// ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒƒãƒ•ã‚¡ (ã‚¨ãƒƒã‚¸ç’°å¢ƒã®åˆ¶ç´„å¯¾å¿œ)
const buffer = new StreamingBuffer(1024, (chunk) => {
  sendToClient(chunk); // 1KBæ¯ã«è‡ªå‹•é€ä¿¡
});

// LRUã‚­ãƒ£ãƒƒã‚·ãƒ¥ã§ãƒ¬ã‚¹ãƒãƒ³ã‚¹æœ€é©åŒ–
const cache = new EdgeCache<string>(100, 5 * 60 * 1000);
const cacheKey = `${model}:${prompt.slice(0, 50)}`;
const cached = cache.get(cacheKey);
if (!cached) {
  const result = await groq.instant(apiKey).generate(prompt);
  cache.set(cacheKey, result.text);
}
```

## ğŸŒŠ WebStreams API

```typescript
import {
  createCloudflareStream,
  cloudflareStreamToText,
  streamToResponse,
  streamToAsyncIterator,
} from "@aid-on/unilmp";

// Cloudflare Workers / Vercel Edge ã§ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
export async function POST(request: Request) {
  const { messages } = await request.json();
  
  // 1. CloudflareStreamChunk ã‚¹ãƒˆãƒªãƒ¼ãƒ ä½œæˆ
  const cloudflareStream = createCloudflareStream(
    "@cf/openai/gpt-oss-120b",
    messages,
    credentials
  );
  
  // 2. ãƒ†ã‚­ã‚¹ãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã«å¤‰æ›
  const textStream = cloudflareStreamToText(cloudflareStream);
  
  // 3. Response ã¨ã—ã¦è¿”å´
  return streamToResponse(textStream, {
    headers: { "X-Model": "gpt-oss-120b" }
  });
}

// AsyncIterator ã¨ã—ã¦æ¶ˆè²»
for await (const chunk of streamToAsyncIterator(cloudflareStream)) {
  console.log(chunk.response); // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ†ã‚­ã‚¹ãƒˆ
  if (chunk.finished) break;
}
```

## ğŸ¯ æ§‹é€ åŒ–å‡ºåŠ›

```typescript
import { generateObject, extractJSON } from "@aid-on/unilmp";
import { z } from "zod";

// ã‚¹ã‚­ãƒ¼ãƒå®šç¾©
const ArticleSchema = z.object({
  title: z.string(),
  sections: z.array(z.object({
    heading: z.string(),
    content: z.string(),
    keywords: z.array(z.string()),
  })),
  metadata: z.object({
    wordCount: z.number(),
    readingTime: z.number(),
    difficulty: z.enum(["beginner", "intermediate", "advanced"]),
  }),
});

// æ§‹é€ åŒ–ç”Ÿæˆ
const article = await generateObject({
  model: "groq:llama-3.3-70b-versatile",
  credentials,
  schema: ArticleSchema,
  prompt: "Write an article about TypeScript",
  system: "You are a technical writer",
});

// å‹å®‰å…¨ã‚¢ã‚¯ã‚»ã‚¹
console.log(article.object.title);           // string
console.log(article.object.sections[0].heading); // string  
console.log(article.object.metadata.difficulty); // "beginner" | "intermediate" | "advanced"

// ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰JSONæŠ½å‡º
const rawResponse = "Here's the data: {\"name\": \"John\", \"age\": 30}";
const person = extractJSON(rawResponse, z.object({
  name: z.string(),
  age: z.number(),
}));
```

## âŒ ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

```typescript
import { 
  LLMProviderError, 
  wrapError, 
  isRetryable,
  withRetry 
} from "@aid-on/unilmp";

try {
  const result = await generate("groq:llama-3.1-8b-instant", messages, credentials);
} catch (error) {
  const llmError = wrapError(error);
  
  console.log(llmError.code);     // 'RATE_LIMITED', 'TIMEOUT', etc.
  console.log(llmError.provider); // 'groq', 'gemini', 'cloudflare'
  console.log(llmError.retryable); // boolean
  
  if (isRetryable(llmError)) {
    // è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤
    const retryResult = await withRetry(
      () => generate("groq:llama-3.1-8b-instant", messages, credentials),
      { maxRetries: 3, baseDelay: 1000 }
    );
  }
}
```

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ

| ç‰¹å¾´ | @aid-on/unilmp | AI SDKç›´æ¥ | @aid-on/unilmp-vercel-ai-sdk |
|------|----------------|-----------|------------------------------|
| ãƒãƒ³ãƒ‰ãƒ«ã‚µã‚¤ã‚º | ~50KB | ~200KB+ | ~200KB+ |
| Cold start | ~10ms | ~50ms+ | ~50ms+ |
| ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ | æœ€å° | å¤§ | å¤§ |
| ä¾å­˜é–¢ä¿‚ | Zod ã®ã¿ | å¤šæ•° | å¤šæ•° |
| ã‚¨ãƒƒã‚¸æœ€é©åŒ– | âœ… ãƒã‚¤ãƒ†ã‚£ãƒ– | âŒ | âœ… äº’æ› |
| å‹å®‰å…¨æ€§ | âœ… Zod | âœ… | âœ… |

## ğŸ”— é–¢é€£ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸

- **[@aid-on/unilmp](.)** - ã‚¨ãƒƒã‚¸ãƒã‚¤ãƒ†ã‚£ãƒ–ã‚³ã‚¢ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆã“ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ï¼‰
- **[@aid-on/unilmp-vercel-ai-sdk](../unilmp-vercel-ai-sdk)** - AI SDKäº’æ›ãƒ©ãƒƒãƒ‘ãƒ¼

## ğŸ¯ ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¬ã‚¤ãƒ‰

### AI SDKã‹ã‚‰ç§»è¡Œ

```typescript
// Before (AI SDK)
import { generateText } from "ai";
import { createGroq } from "@ai-sdk/groq";

const groq = createGroq({ apiKey: "..." });
const result = await generateText({
  model: groq("llama-3.1-8b-instant"),
  prompt: "Hello",
});

// After (unilmp edge-native)
import { generate } from "@aid-on/unilmp";

const result = await generate(
  "groq:llama-3.1-8b-instant",
  [{ role: "user", content: "Hello" }],
  { groqApiKey: "..." }
);
```

### æ®µéšçš„ç§»è¡Œ

AI SDKäº’æ›æ€§ãŒå¿…è¦ãªå ´åˆã¯ `@aid-on/unilmp-vercel-ai-sdk` ã‚’ä½¿ç”¨ï¼š

```typescript
// æ®µéš1: AI SDKäº’æ›ãƒ©ãƒƒãƒ‘ãƒ¼ä½¿ç”¨
import { getModel } from "@aid-on/unilmp-vercel-ai-sdk";
import { generateText } from "ai";

const model = getModel("groq:llama-3.1-8b-instant", credentials);
const result = await generateText({ model, prompt: "Hello" });

// æ®µéš2: å®Œå…¨ã‚¨ãƒƒã‚¸ãƒã‚¤ãƒ†ã‚£ãƒ–ç§»è¡Œ
import { generate } from "@aid-on/unilmp";

const result = await generate(
  "groq:llama-3.1-8b-instant", 
  [{ role: "user", content: "Hello" }], 
  credentials
);
```

## ğŸ“š é«˜åº¦ãªä½¿ç”¨ä¾‹

### ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç’°å¢ƒã§ã®å®Ÿè£…

```typescript
import { 
  unilmp,
  groq,
  EdgeCache,
  getMemoryEstimate,
} from "@aid-on/unilmp";

// ãƒªãƒˆãƒ©ã‚¤ä»˜ãã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¬ã‚¤ãƒ¤ãƒ¼
const responseCache = new EdgeCache<string>(1000, 10 * 60 * 1000);

async function intelligentGenerate(
  userInput: string,
  conversation: Array<{ role: string; content: string }>,
  credentials: any
) {
  // 1. ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
  const memory = getMemoryEstimate();
  console.log(`Memory usage: ${memory.heapUsed}MB`);
  
  // 2. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
  const cacheKey = `${userInput.slice(0, 50)}:${conversation.length}`;
  const cached = responseCache.get(cacheKey);
  if (cached) return { text: cached };
  
  // 3. Fluent API ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãç”Ÿæˆ
  let result;
  try {
    // ä¸»åŠ›ãƒ¢ãƒ‡ãƒ« (è‡ªå‹•æœ€é©åŒ–ãƒ»ãƒªãƒˆãƒ©ã‚¤ä»˜ã)
    result = await unilmp()
      .model("groq:llama-3.3-70b-versatile")
      .credentials(credentials)
      .messages(conversation)
      .optimize(4000) // 4K tokensä»¥å†…ã«è‡ªå‹•å‰Šæ¸›
      .compress() // ç©ºç™½åœ§ç¸®
      .system("You are a helpful AI assistant")
      .temp(0.7)
      .retries(2, 500) // 2å›ãƒªãƒˆãƒ©ã‚¤ã€500msé–“éš”
      .generate(userInput);
  } catch (error) {
    console.warn("Primary model failed, using fallback");
    // é«˜é€Ÿãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    result = await groq.instant(credentials.groqApiKey)
      .temp(0.5)
      .generate(userInput);
  }
  
  // 4. çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
  responseCache.set(cacheKey, result.text);
  
  return result;
}
```

### ã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹é–¢æ•°ã§ã®æœ€é©åŒ–

```typescript
// Cloudflare Workers
export default {
  async fetch(request: Request, env: Env): Promise<Response> {
    try {
      const { prompt } = await request.json();
      
      // æ§‹é€ åŒ–å¿œç­”ã§ç¢ºå®ŸãªJSONè¿”å´
      const analysis = await generateObject({
        model: "cloudflare:@cf/openai/gpt-oss-120b",
        credentials: {
          cloudflareApiKey: env.CLOUDFLARE_API_KEY,
          cloudflareEmail: env.CLOUDFLARE_EMAIL,
          cloudflareAccountId: env.CLOUDFLARE_ACCOUNT_ID,
        },
        schema: z.object({
          response: z.string(),
          sentiment: z.enum(["positive", "negative", "neutral"]),
          confidence: z.number().min(0).max(1),
        }),
        prompt,
        system: "Analyze and respond concisely",
      });
      
      return Response.json({
        ...analysis.object,
        usage: analysis.usage,
        provider: "cloudflare:gpt-oss-120b",
      });
    } catch (error) {
      return Response.json(
        { error: error instanceof Error ? error.message : "Unknown error" },
        { status: 500 }
      );
    }
  }
};
```

## ğŸ“Š API ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹

### ğŸ”§ ã‚³ã‚¢é–¢æ•°

| é–¢æ•° | èª¬æ˜ |
|------|------|
| `generate(spec, messages, credentials, options?)` | ä»»æ„ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ |
| `generateWithGroq(model, messages, apiKey, options?)` | Groqç›´æ¥å‘¼ã³å‡ºã— |
| `generateWithGemini(model, messages, apiKey, options?)` | Geminiç›´æ¥å‘¼ã³å‡ºã— |
| `generateObject(options)` | Zodã‚¹ã‚­ãƒ¼ãƒä»˜ãæ§‹é€ åŒ–ç”Ÿæˆ |

### ğŸŒŠ WebStreams

| é–¢æ•° | èª¬æ˜ |
|------|------|
| `createCloudflareStream(model, messages, creds)` | ReadableStream<CloudflareStreamChunk> |
| `cloudflareStreamToText(stream)` | ReadableStream<string> ã«å¤‰æ› |
| `streamToResponse(stream, init?)` | Response ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ |
| `streamToAsyncIterator(stream)` | AsyncIterator å¤‰æ› |

### ğŸ’¾ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–

| é–¢æ•° | èª¬æ˜ |
|------|------|
| `truncateMessages(messages, maxTokens)` | ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´å‰Šæ¸› |
| `compressMessage(content)` | ç©ºç™½æ–‡å­—åœ§ç¸® |
| `StreamingBuffer(maxSize, onFlush?)` | è‡ªå‹•flushãƒãƒƒãƒ•ã‚¡ |
| `EdgeCache<T>(maxSize, ttl)` | LRUã‚­ãƒ£ãƒƒã‚·ãƒ¥ |
| `getMemoryEstimate()` | ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å–å¾— |

### ğŸ” ãƒªãƒˆãƒ©ã‚¤ãƒ»ã‚¨ãƒ©ãƒ¼

| é–¢æ•° | èª¬æ˜ |
|------|------|
| `withRetry(fn, config)` | æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ãƒªãƒˆãƒ©ã‚¤ |
| `wrapError(error, provider?, context?)` | çµ±ä¸€ã‚¨ãƒ©ãƒ¼å½¢å¼ |
| `isRetryable(error)` | ãƒªãƒˆãƒ©ã‚¤å¯èƒ½åˆ¤å®š |

### ğŸ“‹ ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£

| é–¢æ•° | èª¬æ˜ |
|------|------|
| `parseModelSpec(spec)` | "provider:model" ã‚’ãƒ‘ãƒ¼ã‚¹ |
| `createModelSpec(provider, model)` | specæ–‡å­—åˆ—ä½œæˆ |
| `hasCredentials(provider, creds)` | èªè¨¼æƒ…å ±ç¢ºèª |
| `getCredentialsFromEnv()` | ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èªè¨¼æƒ…å ±å–å¾— |
| `getModelInfo(spec)` | ãƒ¢ãƒ‡ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾— |
| `getAllSpecs()` | å…¨ãƒ¢ãƒ‡ãƒ«specä¸€è¦§ |

## ğŸ† ãƒ¢ãƒ‡ãƒ«é¸æŠã‚¬ã‚¤ãƒ‰

### ğŸ¯ ç”¨é€”åˆ¥æ¨å¥¨ãƒ¢ãƒ‡ãƒ«

```typescript
// âš¡ è¶…é«˜é€Ÿå¿œç­” (ãƒãƒ£ãƒƒãƒˆã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ )
await groq.instant(key).generate(prompt);              // 560 tokens/sec

// ğŸ§  ãƒãƒ©ãƒ³ã‚¹é‡è¦– (æ±ç”¨ã‚¿ã‚¹ã‚¯)
await groq.versatile(key).generate(prompt);            // 280 tokens/sec
await gemini.flash(key).generate(prompt);              // é«˜å“è³ª

// ğŸ† æœ€é«˜æ€§èƒ½ (è¤‡é›‘ãªã‚¿ã‚¹ã‚¯)
await groq.gpt120b(key).generate(prompt);              // 120B parameters
await gemini.pro3(key).generate(prompt);               // æœ€æ–°Gemini 3

// ğŸ’» ã‚³ãƒ¼ãƒ‰ç”Ÿæˆãƒ»ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°
await cloudflare.coder(creds).generate(prompt);       // Qwen Coderç‰¹åŒ–
await groq.compound(key).generate(prompt);             // Webæ¤œç´¢+å®Ÿè¡Œ

// ğŸ¤” æ¨è«–ãƒ»æ•°å­¦ãƒ»è«–ç†å•é¡Œ
await cloudflare.reasoning(creds).generate(prompt);   // QwQæ¨è«–ç‰¹åŒ–
await groq.gpt120b(key).generate(prompt);             // é«˜æ€§èƒ½æ¨è«–

// ğŸ¨ ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ« (ç”»åƒãƒ»éŸ³å£°)
await cloudflare.llama4(creds).generate(prompt);      // Llama 4 Scout
await gemini.flash3(key).generate(prompt);            // Gemini 3

// ğŸ’° ã‚³ã‚¹ãƒˆæœ€é©åŒ– (å¤§é‡å‡¦ç†)
await cloudflare.llama8b(creds).generate(prompt);     // ç„¡æ–™ãƒ»è»½é‡
await gemini.lite(key).generate(prompt);              // ä½ã‚³ã‚¹ãƒˆ

// ğŸ›¡ï¸ ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒ»ãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
await groq.guard(key).generate(prompt);               // Llama Guardå°‚ç”¨

// ğŸ¢ ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºãƒ»ä¿¡é ¼æ€§
await cloudflare.granite(creds).generate(prompt);     // IBM Granite
await gemini.pro(key).generate(prompt);               // å®‰å®šç‰ˆPro
```

### ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ

| ç”¨é€” | 1st Choice | 2nd Choice | 3rd Choice |
|------|-----------|-----------|-----------|
| âš¡ é«˜é€Ÿå¿œç­” | `groq.instant` (560 tok/s) | `groq.gpt20b` (1000 tok/s) | `gemini.lite` |
| ğŸ§  æ±ç”¨ã‚¿ã‚¹ã‚¯ | `groq.versatile` | `gemini.flash` | `cloudflare.llama70b` |
| ğŸ† æœ€é«˜æ€§èƒ½ | `groq.gpt120b` | `gemini.pro3` | `cloudflare.gpt120b` |
| ğŸ’» ã‚³ãƒ¼ãƒ‰ | `cloudflare.coder` | `groq.compound` | `groq.gpt120b` |
| ğŸ¤” æ¨è«– | `cloudflare.reasoning` | `groq.gpt120b` | `gemini.pro25` |
| ğŸ’° ã‚³ã‚¹ãƒˆ | `cloudflare.llama8b` | `gemini.lite` | `groq.instant` |

### 2. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æˆ¦ç•¥

```typescript
// æ¨å¥¨ãƒ‘ã‚¿ãƒ¼ãƒ³
try {
  return await withRetry(
    () => generate(primaryModel, messages, credentials),
    { maxRetries: 2, baseDelay: 500 }
  );
} catch (error) {
  // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
  return await generate(fallbackModel, messages, credentials);
}
```

### 3. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–

```typescript
// ã‚¨ãƒƒã‚¸ç’°å¢ƒã§ã®æ¨å¥¨è¨­å®š
const optimizedMessages = truncateMessages(messages, 2000); // 2K tokens
const cache = new EdgeCache(50, 5 * 60 * 1000); // 50é …ç›®, 5åˆ†
const buffer = new StreamingBuffer(512); // 512B buffer
```

## ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

MIT

---

## ğŸŒŸ v0.3.0 æ–°æ©Ÿèƒ½

### âœ¨ Fluent Builder API
```typescript
// ğŸ¨ ç¾ã—ã„ãƒã‚§ã‚¤ãƒ³å¼API
const result = await unilmp()
  .model("groq:llama-3.3-70b-versatile")
  .credentials(creds)
  .system("You are a coding assistant")
  .temp(0.5)
  .retries(3)
  .optimize(4000)
  .compress()
  .generate("Create a React component");

// ğŸš€ ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆ
await groq.compound(key).generate("Search and code");
await gemini.flash3(key).generate("Latest AI model");
await cloudflare.reasoning(creds).generate("Solve this logic puzzle");
```

### ğŸ“Š **38ãƒ¢ãƒ‡ãƒ«ç¶²ç¾…** 
- **Groq**: 7ãƒ¢ãƒ‡ãƒ« (è¶…é«˜é€Ÿã€œWebæ¤œç´¢+ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œ)
- **Gemini**: 9ãƒ¢ãƒ‡ãƒ« (Gemini 3ã€œ1.5 å…¨ã‚·ãƒªãƒ¼ã‚º)  
- **Cloudflare**: 22ãƒ¢ãƒ‡ãƒ« (OpenAI OSSã€œç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«)

### ğŸ¯ **ç”¨é€”ç‰¹åŒ–API**
- `groq.instant()` - 560 tokens/sec æœ€é«˜é€Ÿ
- `cloudflare.coder()` - ã‚³ãƒ¼ãƒ‰ç”Ÿæˆç‰¹åŒ–
- `cloudflare.reasoning()` - æ¨è«–å•é¡Œç‰¹åŒ–
- `gemini.pro3()` - æœ€æ–°æœ€é«˜å“è³ª
- `groq.compound()` - Webæ¤œç´¢+ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œ

---

**@aid-on/unilmp** ã§æ¬¡ä¸–ä»£ã®ã‚¨ãƒƒã‚¸ãƒã‚¤ãƒ†ã‚£ãƒ–AIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰ã—ã¾ã—ã‚‡ã†ï¼ğŸš€